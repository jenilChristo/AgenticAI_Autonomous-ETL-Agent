{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Churn Prediction ETL Pipeline üìä\n\n",
        "**Description:** Create a comprehensive ETL pipeline that:\n\n1. Ingests customer transaction data from multiple CSV files\n2. Performs data cleaning and feature engineering \n3. Calculates customer behavior metrics (RFM analysis, transaction patterns)\n4. Applies machine learning to predict customer churn risk\n5. Exports results to Parquet format for downstream ML models\n\nRequirements:\n- Handle files up to 500MB with millions of records\n- Implement data quality checks and validation\n- Use PySpark for distributed processing\n- Include comprehensive error handling and logging\n- Generate summary statistics and data profiling reports\n\n\n",
        "**Generated:** 2025-10-03 00:14:26\n\n",
        "**Tasks Covered:** 5 tasks\n\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Amazon Senior Data Engineer - Customer Acquisition Analytics\n",
        "# Production-ready Databricks notebook for customer acquisition metrics\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from delta import *\n",
        "import boto3\n",
        "from datetime import datetime, timedelta\n",
        "import logging\n",
        "\n",
        "# Configure logging for production\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize Spark session with Delta Lake and production configurations\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('CustomerAcquisitionAnalytics') \\\n",
        "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \\\n",
        "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog') \\\n",
        "    .config('spark.sql.adaptive.enabled', 'true') \\\n",
        "    .config('spark.sql.adaptive.coalescePartitions.enabled', 'true') \\\n",
        "    .config('spark.databricks.delta.optimizeWrite.enabled', 'true') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbose output\n",
        "spark.sparkContext.setLogLevel('WARN')\n",
        "\n",
        "print('‚úÖ Spark session initialized for customer acquisition analytics')\n",
        "print(f'üîß Spark version: {spark.version}')\n",
        "print(f'üìä Available cores: {spark.sparkContext.defaultParallelism}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-1"
        ]
      },
      "source": [
        "## Task 1: Task 1 üéØ\n\n",
        "<div style='background-color: #e7f3ff; padding: 15px; border-left: 4px solid #2196F3;'>\n",
        "<h3>üìã Task Overview</h3>\n",
        "<ul>\n",
        "<li><strong>Type:</strong> pyspark</li>\n",
        "<li><strong>Priority:</strong> high</li>\n",
        "<li><strong>Estimated Effort:</strong> medium</li>\n",
        "</ul>\n",
        "<p><strong>Description:</strong> Develop a PySpark job to read and consolidate multiple large CSV files containing customer transaction data into a single DataFrame. Implement robust error handling and logging for file read operations, and ensure schema consistency across files.</p>\n",
        "</div>\n\n",
        "### üéì Learning Objectives\n",
        "In this task, you will learn:\n",
        "- Production-ready PySpark patterns for pyspark\n",
        "- Error handling and logging best practices\n",
        "- Performance optimization techniques for large datasets\n",
        "- Data quality validation methods\n",
        "- Amazon-scale data engineering patterns\n\n",
        "### üèóÔ∏è Architecture Pattern\n",
        "This task follows the **Medallion Architecture** pattern:\n",
        "- **Bronze Layer:** Raw data ingestion with minimal processing\n",
        "- **Silver Layer:** Cleaned and validated data with business rules applied\n",
        "- **Gold Layer:** Aggregated metrics ready for analytics and reporting\n\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "task-1-processing"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Task 1: Task 1\n",
        "# Develop a PySpark job to read and consolidate multiple large CSV files containing customer transaction data into a single DataFrame. Implement robust error handling and logging for file read operations, and ensure schema consistency across files.\n",
        "\n",
        "try:\n",
        "    logger.info(f'üîÑ Starting task: {task.get(\"description\", \"Data processing\")}')\n",
        "    \n",
        "    # Generic processing based on available data\n",
        "    if 'df_cleaned' in locals():\n",
        "        df_processed = df_cleaned\n",
        "    elif 'df_raw_events' in locals():\n",
        "        df_processed = df_raw_events\n",
        "    else:\n",
        "        raise ValueError('No source dataframe available for processing')\n",
        "    \n",
        "    # Add common processing steps\n",
        "    df_task_result = df_processed \\\n",
        "        .filter(col('customer_id').isNotNull()) \\\n",
        "        .withColumn('processed_timestamp', current_timestamp()) \\\n",
        "        .withColumn('processing_batch_id', lit(f'task_{task_number}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'))\n",
        "    \n",
        "    # Show results\n",
        "    record_count = df_task_result.count()\n",
        "    \n",
        "    print(f'‚úÖ Task {task_number} Processing Completed:')\n",
        "    print(f'   üìä Records Processed: {record_count:,}')\n",
        "    print(f'   üìã Task Description: {task.get(\"description\", \"Generic processing\")}')\n",
        "    print(f'   üè∑Ô∏è  Task Type: {task.get(\"task_type\", \"processing\")}')\n",
        "    \n",
        "    # Display sample results\n",
        "    print(f'\\nüìã Sample Results:')\n",
        "    df_task_result.select('customer_id', 'event_type', 'processed_timestamp').show(5)\n",
        "    \n",
        "    logger.info(f'‚úÖ Task {task_number} completed - {record_count:,} records processed')\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f'‚ùå Task {task_number} failed: {str(e)}')\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-1-summary"
        ]
      },
      "source": [
        "### ‚úÖ Task 1 Completed\n\n",
        "Task 'Task 1' has been executed successfully.\n",
        "Check the output above for results and any validation messages.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-2"
        ]
      },
      "source": [
        "## Task 2: Task 2 üéØ\n\n",
        "<div style='background-color: #e7f3ff; padding: 15px; border-left: 4px solid #2196F3;'>\n",
        "<h3>üìã Task Overview</h3>\n",
        "<ul>\n",
        "<li><strong>Type:</strong> pyspark</li>\n",
        "<li><strong>Priority:</strong> high</li>\n",
        "<li><strong>Estimated Effort:</strong> large</li>\n",
        "</ul>\n",
        "<p><strong>Description:</strong> Implement data cleaning steps (e.g., handling missing values, removing duplicates, correcting data types) and perform feature engineering, including calculation of RFM (Recency, Frequency, Monetary) metrics and transaction pattern features using PySpark transformations.</p>\n",
        "</div>\n\n",
        "### üéì Learning Objectives\n",
        "In this task, you will learn:\n",
        "- Production-ready PySpark patterns for pyspark\n",
        "- Error handling and logging best practices\n",
        "- Performance optimization techniques for large datasets\n",
        "- Data quality validation methods\n",
        "- Amazon-scale data engineering patterns\n\n",
        "### üèóÔ∏è Architecture Pattern\n",
        "This task follows the **Medallion Architecture** pattern:\n",
        "- **Bronze Layer:** Raw data ingestion with minimal processing\n",
        "- **Silver Layer:** Cleaned and validated data with business rules applied\n",
        "- **Gold Layer:** Aggregated metrics ready for analytics and reporting\n\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-2-transform-intro"
        ]
      },
      "source": [
        "### üîÑ Data Transformation Deep Dive\n\n",
        "<div style='background-color: #fff3e0; padding: 15px; border-left: 4px solid #ff9800;'>\n",
        "<h4>üéì Transformation Learning Objectives</h4>\n",
        "<p>Data transformation is where raw data becomes business-ready insights. You'll learn:</p>\n",
        "<ul>\n",
        "<li><strong>Data Cleaning:</strong> Null handling, deduplication, and validation</li>\n",
        "<li><strong>Feature Engineering:</strong> Creating derived columns for analytics</li>\n",
        "<li><strong>Business Logic:</strong> Implementing domain-specific rules</li>\n",
        "<li><strong>Performance Patterns:</strong> Efficient transformation techniques</li>\n",
        "</ul>\n",
        "</div>\n\n",
        "### üèóÔ∏è Transformation Pipeline Architecture\n",
        "```\n",
        "Raw Data ‚Üí Data Cleaning ‚Üí Feature Engineering ‚Üí Business Logic ‚Üí Metrics Calculation\n",
        "(Bronze)     (Validation)    (New Columns)      (Rules)         (Aggregations)\n",
        "```\n\n",
        "### üîß PySpark Transformation Best Practices\n",
        "- **Use Column API** (`col()`, `when()`, `otherwise()`) for readable code\n",
        "- **Chain operations** efficiently to minimize shuffles\n",
        "- **Cache intermediate results** that are used multiple times\n",
        "- **Use coalesce()** to handle null values gracefully\n",
        "- **Partition by relevant keys** to optimize joins and aggregations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "task-2-cleaning"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Task 2: Data Cleaning and Preparation\n",
        "# Clean and prepare customer data for analytics\n",
        "\n",
        "try:\n",
        "    logger.info('üßπ Starting data cleaning and preparation')\n",
        "    \n",
        "    # Data cleaning pipeline\n",
        "    df_cleaned = df_raw_events \\\n",
        "        .filter(col('customer_id').isNotNull()) \\\n",
        "        .filter(col('event_date').isNotNull()) \\\n",
        "        .filter(col('revenue') >= 0) \\\n",
        "        .withColumn('year_month', date_format(col('event_date'), 'yyyy-MM')) \\\n",
        "        .withColumn('year', year(col('event_date'))) \\\n",
        "        .withColumn('quarter', quarter(col('event_date'))) \\\n",
        "        .withColumn('acquisition_channel', coalesce(col('channel'), lit('unknown'))) \\\n",
        "        .withColumn('clean_campaign_id', coalesce(col('campaign_id'), lit('organic'))) \\\n",
        "        .withColumn('revenue_bucket', \n",
        "            when(col('revenue') == 0, 'zero')\n",
        "            .when(col('revenue') <= 50, 'low')\n",
        "            .when(col('revenue') <= 200, 'medium')\n",
        "            .when(col('revenue') <= 500, 'high')\n",
        "            .otherwise('premium')\n",
        "        )\n",
        "    \n",
        "    # Remove duplicates based on customer_id, event_type, and event_date\n",
        "    df_cleaned = df_cleaned.dropDuplicates(['customer_id', 'event_type', 'event_date'])\n",
        "    \n",
        "    # Cache cleaned data\n",
        "    df_cleaned.cache()\n",
        "    \n",
        "    cleaned_count = df_cleaned.count()\n",
        "    original_count = df_raw_events.count()\n",
        "    \n",
        "    print(f'‚úÖ Data Cleaning Completed:')\n",
        "    print(f'   üì• Original Records: {original_count:,}')\n",
        "    print(f'   üì§ Cleaned Records: {cleaned_count:,}')\n",
        "    print(f'   üóëÔ∏è  Records Removed: {original_count - cleaned_count:,} ({((original_count - cleaned_count) / original_count * 100):.2f}%)')\n",
        "    \n",
        "    logger.info(f'‚úÖ Data cleaning completed - {cleaned_count:,} clean records')\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f'‚ùå Data cleaning failed: {str(e)}')\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-2-cleaning-explanation"
        ]
      },
      "source": [
        "#### üßπ Data Cleaning Techniques Explained\n\n",
        "<div style='background-color: #f8f9fa; padding: 15px; border-left: 4px solid #6c757d;'>\n",
        "<h5>üîç Cleaning Operations Breakdown</h5>\n",
        "<p>The data cleaning pipeline above implements several critical operations:</p>\n",
        "</div>\n\n",
        "**1. Null Value Filtering:**\n",
        "```python\n",
        ".filter(col('customer_id').isNotNull())  # Remove records without customer ID\n",
        ".filter(col('event_date').isNotNull())   # Ensure all events have dates\n",
        "```\n\n",
        "**2. Business Rule Validation:**\n",
        "```python\n",
        ".filter(col('revenue') >= 0)  # No negative revenue values\n",
        "```\n\n",
        "**3. Feature Engineering:**\n",
        "```python\n",
        ".withColumn('year_month', date_format(col('event_date'), 'yyyy-MM'))  # Time grouping\n",
        ".withColumn('revenue_bucket', when(...))  # Categorical segmentation\n",
        "```\n\n",
        "**4. Data Standardization:**\n",
        "```python\n",
        ".withColumn('acquisition_channel', coalesce(col('channel'), lit('unknown')))\n",
        "# Handles null channels with default value\n",
        "```\n\n",
        "**5. Deduplication:**\n",
        "```python\n",
        ".dropDuplicates(['customer_id', 'event_type', 'event_date'])\n",
        "# Removes duplicate events for same customer on same day\n",
        "```\n\n",
        "üí° **Pro Tip:** Always measure data loss during cleaning to ensure you're not removing valid business data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "task-2-metrics"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Task 2: Customer Acquisition Metrics Calculation\n",
        "# Calculate CAC, LTV, and conversion metrics for Amazon's customer acquisition platform\n",
        "\n",
        "try:\n",
        "    logger.info('üìä Calculating customer acquisition metrics')\n",
        "    \n",
        "    # Calculate Customer Acquisition Cost (CAC) by channel and time period\n",
        "    df_cac_metrics = df_cleaned \\\n",
        "        .filter(col('event_type') == 'acquisition') \\\n",
        "        .groupBy('acquisition_channel', 'clean_campaign_id', 'year_month') \\\n",
        "        .agg(\n",
        "            sum('marketing_spend').alias('total_marketing_spend'),\n",
        "            countDistinct('customer_id').alias('customers_acquired'),\n",
        "            avg('marketing_spend').alias('avg_spend_per_acquisition')\n",
        "        ) \\\n",
        "        .withColumn('cac', \n",
        "            when(col('customers_acquired') > 0, \n",
        "                 col('total_marketing_spend') / col('customers_acquired')\n",
        "            ).otherwise(0)\n",
        "        ) \\\n",
        "        .withColumn('calculated_timestamp', current_timestamp())\n",
        "    \n",
        "    # Calculate Customer Lifetime Value (LTV) metrics\n",
        "    df_ltv_metrics = df_cleaned \\\n",
        "        .filter(col('event_type') == 'purchase') \\\n",
        "        .groupBy('customer_id', 'acquisition_channel') \\\n",
        "        .agg(\n",
        "            sum('revenue').alias('total_revenue'),\n",
        "            count('*').alias('purchase_frequency'),\n",
        "            avg('revenue').alias('avg_order_value'),\n",
        "            min('event_date').alias('first_purchase_date'),\n",
        "            max('event_date').alias('last_purchase_date')\n",
        "        ) \\\n",
        "        .withColumn('customer_tenure_days',\n",
        "            greatest(datediff(col('last_purchase_date'), col('first_purchase_date')), lit(1))\n",
        "        ) \\\n",
        "        .withColumn('ltv_12_month_estimate', \n",
        "            col('total_revenue') * (365.0 / col('customer_tenure_days'))\n",
        "        ) \\\n",
        "        .withColumn('customer_segment',\n",
        "            when(col('total_revenue') >= 1000, 'high_value')\n",
        "            .when(col('total_revenue') >= 500, 'medium_value')\n",
        "            .otherwise('low_value')\n",
        "        )\n",
        "    \n",
        "    # Calculate conversion rates by channel and time period\n",
        "    df_conversion_metrics = df_cleaned \\\n",
        "        .groupBy('acquisition_channel', 'year_month') \\\n",
        "        .agg(\n",
        "            countDistinct('customer_id').alias('total_customers'),\n",
        "            countDistinct(\n",
        "                when(col('event_type') == 'purchase', col('customer_id'))\n",
        "            ).alias('converted_customers')\n",
        "        ) \\\n",
        "        .withColumn('conversion_rate_pct',\n",
        "            when(col('total_customers') > 0,\n",
        "                 (col('converted_customers') / col('total_customers')) * 100\n",
        "            ).otherwise(0)\n",
        "        )\n",
        "    \n",
        "    # Cache metrics for performance\n",
        "    df_cac_metrics.cache()\n",
        "    df_ltv_metrics.cache()\n",
        "    df_conversion_metrics.cache()\n",
        "    \n",
        "    print('‚úÖ Customer Acquisition Metrics Calculated Successfully')\n",
        "    \n",
        "    logger.info('‚úÖ Customer acquisition metrics calculation completed')\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f'‚ùå Metrics calculation failed: {str(e)}')\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-2-metrics-explanation"
        ]
      },
      "source": [
        "#### üìä Customer Acquisition Metrics Deep Dive\n\n",
        "<div style='background-color: #e8f5e8; padding: 15px; border-left: 4px solid #28a745;'>\n",
        "<h5>üí∞ Business Metrics Explained</h5>\n",
        "<p>Understanding these key customer acquisition metrics is crucial for business success:</p>\n",
        "</div>\n\n",
        "**1. Customer Acquisition Cost (CAC):**\n",
        "```\n",
        "CAC = Total Marketing Spend / Number of Customers Acquired\n",
        "```\n",
        "- Measures efficiency of marketing investments\n",
        "- Lower CAC = more efficient acquisition\n",
        "- Should be tracked by channel and campaign\n\n",
        "**2. Customer Lifetime Value (LTV):**\n",
        "```\n",
        "LTV = Total Revenue √ó (365 / Customer Tenure Days)\n",
        "```\n",
        "- Estimates total revenue from a customer over 12 months\n",
        "- Higher LTV = more valuable customers\n",
        "- Used to justify marketing investments\n\n",
        "**3. LTV/CAC Ratio:**\n",
        "```\n",
        "LTV/CAC Ratio = Average LTV / Average CAC\n",
        "```\n",
        "- **> 3.0:** Healthy acquisition channel\n",
        "- **< 1.0:** Losing money on acquisitions\n",
        "- **Optimal:** Between 3.0 and 5.0\n\n",
        "**4. Conversion Rate:**\n",
        "```\n",
        "Conversion Rate = (Converted Customers / Total Customers) √ó 100\n",
        "```\n",
        "- Measures funnel efficiency\n",
        "- Higher conversion = better targeting\n\n",
        "### üéØ Advanced PySpark Aggregation Patterns\n",
        "```python\n",
        "# Window functions for cohort analysis\n",
        ".withColumn('customer_rank', row_number().over(Window.partitionBy('customer_id').orderBy('event_date')))\n\n",
        "# Conditional aggregations\n",
        "countDistinct(when(col('event_type') == 'purchase', col('customer_id')))\n\n",
        "# Percentile calculations\n",
        "percentile_approx('revenue', 0.5).alias('median_revenue')\n",
        "```\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "task-2-results"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Task 2: Display Transformation Results\n",
        "# Show calculated metrics and key insights\n",
        "\n",
        "print('üìä CUSTOMER ACQUISITION COST (CAC) BY CHANNEL:')\n",
        "print('=' * 60)\n",
        "df_cac_metrics.orderBy(desc('cac')).show(20, truncate=False)\n",
        "\n",
        "print('\\nüìà CUSTOMER LIFETIME VALUE (LTV) SUMMARY:')\n",
        "print('=' * 60)\n",
        "df_ltv_metrics.agg(\n",
        "    count('customer_id').alias('total_customers'),\n",
        "    avg('total_revenue').alias('avg_ltv'),\n",
        "    percentile_approx('total_revenue', 0.5).alias('median_ltv'),\n",
        "    max('total_revenue').alias('max_ltv'),\n",
        "    avg('purchase_frequency').alias('avg_purchases_per_customer')\n",
        ").show(truncate=False)\n",
        "\n",
        "print('\\nüéØ CONVERSION RATES BY CHANNEL:')\n",
        "print('=' * 60)\n",
        "df_conversion_metrics.orderBy(desc('conversion_rate_pct')).show(20, truncate=False)\n",
        "\n",
        "print('\\nüí∞ LTV/CAC RATIO BY CHANNEL (Key Business Metric):')\n",
        "print('=' * 60)\n",
        "# Calculate LTV/CAC ratio for business insights\n",
        "ltv_by_channel = df_ltv_metrics.groupBy('acquisition_channel').agg(\n",
        "    avg('ltv_12_month_estimate').alias('avg_ltv_12m')\n",
        ")\n",
        "\n",
        "cac_by_channel = df_cac_metrics.groupBy('acquisition_channel').agg(\n",
        "    avg('cac').alias('avg_cac')\n",
        ")\n",
        "\n",
        "ltv_cac_ratio = ltv_by_channel.join(cac_by_channel, 'acquisition_channel', 'inner') \\\n",
        "    .withColumn('ltv_cac_ratio', \n",
        "        when(col('avg_cac') > 0, col('avg_ltv_12m') / col('avg_cac')).otherwise(0)\n",
        "    ) \\\n",
        "    .orderBy(desc('ltv_cac_ratio'))\n",
        "\n",
        "ltv_cac_ratio.show(truncate=False)\n",
        "\n",
        "print('\\nüìã Business Insights:')\n",
        "print('   ‚Ä¢ LTV/CAC ratio > 3.0 indicates healthy acquisition channels')\n",
        "print('   ‚Ä¢ Focus investment on channels with highest LTV/CAC ratios')\n",
        "print('   ‚Ä¢ Monitor conversion rates and optimize underperforming channels')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-2-summary"
        ]
      },
      "source": [
        "### ‚úÖ Task 2 Completed\n\n",
        "Task 'Task 2' has been executed successfully.\n",
        "Check the output above for results and any validation messages.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-3"
        ]
      },
      "source": [
        "## Task 3: Task 3 üéØ\n\n",
        "<div style='background-color: #e7f3ff; padding: 15px; border-left: 4px solid #2196F3;'>\n",
        "<h3>üìã Task Overview</h3>\n",
        "<ul>\n",
        "<li><strong>Type:</strong> pyspark</li>\n",
        "<li><strong>Priority:</strong> high</li>\n",
        "<li><strong>Estimated Effort:</strong> medium</li>\n",
        "</ul>\n",
        "<p><strong>Description:</strong> Create PySpark routines to validate data quality (e.g., null checks, range checks, uniqueness constraints) and generate summary statistics and data profiling reports. Log all validation results and flag any data quality issues for review.</p>\n",
        "</div>\n\n",
        "### üéì Learning Objectives\n",
        "In this task, you will learn:\n",
        "- Production-ready PySpark patterns for pyspark\n",
        "- Error handling and logging best practices\n",
        "- Performance optimization techniques for large datasets\n",
        "- Data quality validation methods\n",
        "- Amazon-scale data engineering patterns\n\n",
        "### üèóÔ∏è Architecture Pattern\n",
        "This task follows the **Medallion Architecture** pattern:\n",
        "- **Bronze Layer:** Raw data ingestion with minimal processing\n",
        "- **Silver Layer:** Cleaned and validated data with business rules applied\n",
        "- **Gold Layer:** Aggregated metrics ready for analytics and reporting\n\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "task-3-validation"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Task 3: Data Quality Validation\n",
        "# Comprehensive validation for customer acquisition data\n",
        "\n",
        "try:\n",
        "    logger.info('üîç Starting comprehensive data quality validation')\n",
        "    \n",
        "    # Data quality validation suite\n",
        "    validation_results = {}\n",
        "    \n",
        "    # 1. Completeness checks\n",
        "    total_records = df_cleaned.count()\n",
        "    null_customer_ids = df_cleaned.filter(col('customer_id').isNull()).count()\n",
        "    null_event_dates = df_cleaned.filter(col('event_date').isNull()).count()\n",
        "    \n",
        "    validation_results['completeness'] = {\n",
        "        'total_records': total_records,\n",
        "        'null_customer_ids': null_customer_ids,\n",
        "        'null_event_dates': null_event_dates,\n",
        "        'completeness_rate': ((total_records - null_customer_ids) / total_records) * 100\n",
        "    }\n",
        "    \n",
        "    # 2. Validity checks\n",
        "    negative_revenues = df_cleaned.filter(col('revenue') < 0).count()\n",
        "    future_events = df_cleaned.filter(col('event_date') > current_timestamp()).count()\n",
        "    invalid_channels = df_cleaned.filter(col('acquisition_channel').isin(['', ' ', 'null'])).count()\n",
        "    \n",
        "    validation_results['validity'] = {\n",
        "        'negative_revenues': negative_revenues,\n",
        "        'future_events': future_events,\n",
        "        'invalid_channels': invalid_channels\n",
        "    }\n",
        "    \n",
        "    # 3. Consistency checks\n",
        "    duplicate_events = df_cleaned.count() - df_cleaned.dropDuplicates(['customer_id', 'event_type', 'event_date']).count()\n",
        "    \n",
        "    validation_results['consistency'] = {\n",
        "        'duplicate_events': duplicate_events\n",
        "    }\n",
        "    \n",
        "    # 4. Business rule validations\n",
        "    acquisition_without_spend = df_cleaned.filter(\n",
        "        (col('event_type') == 'acquisition') & \n",
        "        ((col('marketing_spend').isNull()) | (col('marketing_spend') <= 0))\n",
        "    ).count()\n",
        "    \n",
        "    purchases_without_revenue = df_cleaned.filter(\n",
        "        (col('event_type') == 'purchase') & \n",
        "        ((col('revenue').isNull()) | (col('revenue') <= 0))\n",
        "    ).count()\n",
        "    \n",
        "    validation_results['business_rules'] = {\n",
        "        'acquisition_without_spend': acquisition_without_spend,\n",
        "        'purchases_without_revenue': purchases_without_revenue\n",
        "    }\n",
        "    \n",
        "    print('üîç DATA QUALITY VALIDATION REPORT')\n",
        "    print('=' * 60)\n",
        "    \n",
        "    print(f'\\nüìä COMPLETENESS METRICS:')\n",
        "    comp = validation_results['completeness']\n",
        "    print(f'   Total Records: {comp[\"total_records\"]:,}')\n",
        "    print(f'   Null Customer IDs: {comp[\"null_customer_ids\"]:,}')\n",
        "    print(f'   Completeness Rate: {comp[\"completeness_rate\"]:.2f}%')\n",
        "    \n",
        "    print(f'\\n‚úÖ VALIDITY CHECKS:')\n",
        "    val = validation_results['validity']\n",
        "    print(f'   Negative Revenues: {val[\"negative_revenues\"]:,}')\n",
        "    print(f'   Future Events: {val[\"future_events\"]:,}')\n",
        "    print(f'   Invalid Channels: {val[\"invalid_channels\"]:,}')\n",
        "    \n",
        "    print(f'\\nüîÑ CONSISTENCY CHECKS:')\n",
        "    cons = validation_results['consistency']\n",
        "    print(f'   Duplicate Events: {cons[\"duplicate_events\"]:,}')\n",
        "    \n",
        "    print(f'\\nüíº BUSINESS RULE VALIDATION:')\n",
        "    biz = validation_results['business_rules']\n",
        "    print(f'   Acquisitions without Marketing Spend: {biz[\"acquisition_without_spend\"]:,}')\n",
        "    print(f'   Purchases without Revenue: {biz[\"purchases_without_revenue\"]:,}')\n",
        "    \n",
        "    # Overall data quality score\n",
        "    quality_score = (\n",
        "        validation_results['completeness']['completeness_rate'] * 0.4 +\n",
        "        (100 - (val['negative_revenues'] + val['future_events']) / total_records * 100) * 0.3 +\n",
        "        (100 - cons['duplicate_events'] / total_records * 100) * 0.3\n",
        "    )\n",
        "    \n",
        "    print(f'\\nüèÜ OVERALL DATA QUALITY SCORE: {quality_score:.1f}/100')\n",
        "    \n",
        "    if quality_score >= 95:\n",
        "        print('   ‚úÖ EXCELLENT - Data ready for production analytics')\n",
        "    elif quality_score >= 85:\n",
        "        print('   ‚ö†Ô∏è  GOOD - Minor quality issues detected')\n",
        "    else:\n",
        "        print('   ‚ùå POOR - Significant data quality issues require attention')\n",
        "    \n",
        "    logger.info(f'‚úÖ Data quality validation completed - Score: {quality_score:.1f}/100')\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f'‚ùå Data validation failed: {str(e)}')\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-3-summary"
        ]
      },
      "source": [
        "### ‚úÖ Task 3 Completed\n\n",
        "Task 'Task 3' has been executed successfully.\n",
        "Check the output above for results and any validation messages.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-4"
        ]
      },
      "source": [
        "## Task 4: Task 4 üéØ\n\n",
        "<div style='background-color: #e7f3ff; padding: 15px; border-left: 4px solid #2196F3;'>\n",
        "<h3>üìã Task Overview</h3>\n",
        "<ul>\n",
        "<li><strong>Type:</strong> pyspark</li>\n",
        "<li><strong>Priority:</strong> medium</li>\n",
        "<li><strong>Estimated Effort:</strong> small</li>\n",
        "</ul>\n",
        "<p><strong>Description:</strong> After applying the churn prediction model, export the resulting DataFrame with customer churn risk scores to Parquet format for downstream ML consumption. Ensure partitioning and efficient storage for large datasets.</p>\n",
        "</div>\n\n",
        "### üéì Learning Objectives\n",
        "In this task, you will learn:\n",
        "- Production-ready PySpark patterns for pyspark\n",
        "- Error handling and logging best practices\n",
        "- Performance optimization techniques for large datasets\n",
        "- Data quality validation methods\n",
        "- Amazon-scale data engineering patterns\n\n",
        "### üèóÔ∏è Architecture Pattern\n",
        "This task follows the **Medallion Architecture** pattern:\n",
        "- **Bronze Layer:** Raw data ingestion with minimal processing\n",
        "- **Silver Layer:** Cleaned and validated data with business rules applied\n",
        "- **Gold Layer:** Aggregated metrics ready for analytics and reporting\n\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "task-4-processing"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Task 4: Task 4\n",
        "# After applying the churn prediction model, export the resulting DataFrame with customer churn risk scores to Parquet format for downstream ML consumption. Ensure partitioning and efficient storage for large datasets.\n",
        "\n",
        "try:\n",
        "    logger.info(f'üîÑ Starting task: {task.get(\"description\", \"Data processing\")}')\n",
        "    \n",
        "    # Generic processing based on available data\n",
        "    if 'df_cleaned' in locals():\n",
        "        df_processed = df_cleaned\n",
        "    elif 'df_raw_events' in locals():\n",
        "        df_processed = df_raw_events\n",
        "    else:\n",
        "        raise ValueError('No source dataframe available for processing')\n",
        "    \n",
        "    # Add common processing steps\n",
        "    df_task_result = df_processed \\\n",
        "        .filter(col('customer_id').isNotNull()) \\\n",
        "        .withColumn('processed_timestamp', current_timestamp()) \\\n",
        "        .withColumn('processing_batch_id', lit(f'task_{task_number}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'))\n",
        "    \n",
        "    # Show results\n",
        "    record_count = df_task_result.count()\n",
        "    \n",
        "    print(f'‚úÖ Task {task_number} Processing Completed:')\n",
        "    print(f'   üìä Records Processed: {record_count:,}')\n",
        "    print(f'   üìã Task Description: {task.get(\"description\", \"Generic processing\")}')\n",
        "    print(f'   üè∑Ô∏è  Task Type: {task.get(\"task_type\", \"processing\")}')\n",
        "    \n",
        "    # Display sample results\n",
        "    print(f'\\nüìã Sample Results:')\n",
        "    df_task_result.select('customer_id', 'event_type', 'processed_timestamp').show(5)\n",
        "    \n",
        "    logger.info(f'‚úÖ Task {task_number} completed - {record_count:,} records processed')\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f'‚ùå Task {task_number} failed: {str(e)}')\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-4-summary"
        ]
      },
      "source": [
        "### ‚úÖ Task 4 Completed\n\n",
        "Task 'Task 4' has been executed successfully.\n",
        "Check the output above for results and any validation messages.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-5"
        ]
      },
      "source": [
        "## Task 5: Task 5 üéØ\n\n",
        "<div style='background-color: #e7f3ff; padding: 15px; border-left: 4px solid #2196F3;'>\n",
        "<h3>üìã Task Overview</h3>\n",
        "<ul>\n",
        "<li><strong>Type:</strong> pyspark</li>\n",
        "<li><strong>Priority:</strong> high</li>\n",
        "<li><strong>Estimated Effort:</strong> medium</li>\n",
        "</ul>\n",
        "<p><strong>Description:</strong> Develop comprehensive tests for each ETL pipeline stage using PySpark, including data ingestion, transformation, validation, and output. Implement detailed logging and error handling to monitor pipeline execution and facilitate troubleshooting.</p>\n",
        "</div>\n\n",
        "### üéì Learning Objectives\n",
        "In this task, you will learn:\n",
        "- Production-ready PySpark patterns for pyspark\n",
        "- Error handling and logging best practices\n",
        "- Performance optimization techniques for large datasets\n",
        "- Data quality validation methods\n",
        "- Amazon-scale data engineering patterns\n\n",
        "### üèóÔ∏è Architecture Pattern\n",
        "This task follows the **Medallion Architecture** pattern:\n",
        "- **Bronze Layer:** Raw data ingestion with minimal processing\n",
        "- **Silver Layer:** Cleaned and validated data with business rules applied\n",
        "- **Gold Layer:** Aggregated metrics ready for analytics and reporting\n\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-5-transform-intro"
        ]
      },
      "source": [
        "### üîÑ Data Transformation Deep Dive\n\n",
        "<div style='background-color: #fff3e0; padding: 15px; border-left: 4px solid #ff9800;'>\n",
        "<h4>üéì Transformation Learning Objectives</h4>\n",
        "<p>Data transformation is where raw data becomes business-ready insights. You'll learn:</p>\n",
        "<ul>\n",
        "<li><strong>Data Cleaning:</strong> Null handling, deduplication, and validation</li>\n",
        "<li><strong>Feature Engineering:</strong> Creating derived columns for analytics</li>\n",
        "<li><strong>Business Logic:</strong> Implementing domain-specific rules</li>\n",
        "<li><strong>Performance Patterns:</strong> Efficient transformation techniques</li>\n",
        "</ul>\n",
        "</div>\n\n",
        "### üèóÔ∏è Transformation Pipeline Architecture\n",
        "```\n",
        "Raw Data ‚Üí Data Cleaning ‚Üí Feature Engineering ‚Üí Business Logic ‚Üí Metrics Calculation\n",
        "(Bronze)     (Validation)    (New Columns)      (Rules)         (Aggregations)\n",
        "```\n\n",
        "### üîß PySpark Transformation Best Practices\n",
        "- **Use Column API** (`col()`, `when()`, `otherwise()`) for readable code\n",
        "- **Chain operations** efficiently to minimize shuffles\n",
        "- **Cache intermediate results** that are used multiple times\n",
        "- **Use coalesce()** to handle null values gracefully\n",
        "- **Partition by relevant keys** to optimize joins and aggregations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "task-5-cleaning"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Task 5: Data Cleaning and Preparation\n",
        "# Clean and prepare customer data for analytics\n",
        "\n",
        "try:\n",
        "    logger.info('üßπ Starting data cleaning and preparation')\n",
        "    \n",
        "    # Data cleaning pipeline\n",
        "    df_cleaned = df_raw_events \\\n",
        "        .filter(col('customer_id').isNotNull()) \\\n",
        "        .filter(col('event_date').isNotNull()) \\\n",
        "        .filter(col('revenue') >= 0) \\\n",
        "        .withColumn('year_month', date_format(col('event_date'), 'yyyy-MM')) \\\n",
        "        .withColumn('year', year(col('event_date'))) \\\n",
        "        .withColumn('quarter', quarter(col('event_date'))) \\\n",
        "        .withColumn('acquisition_channel', coalesce(col('channel'), lit('unknown'))) \\\n",
        "        .withColumn('clean_campaign_id', coalesce(col('campaign_id'), lit('organic'))) \\\n",
        "        .withColumn('revenue_bucket', \n",
        "            when(col('revenue') == 0, 'zero')\n",
        "            .when(col('revenue') <= 50, 'low')\n",
        "            .when(col('revenue') <= 200, 'medium')\n",
        "            .when(col('revenue') <= 500, 'high')\n",
        "            .otherwise('premium')\n",
        "        )\n",
        "    \n",
        "    # Remove duplicates based on customer_id, event_type, and event_date\n",
        "    df_cleaned = df_cleaned.dropDuplicates(['customer_id', 'event_type', 'event_date'])\n",
        "    \n",
        "    # Cache cleaned data\n",
        "    df_cleaned.cache()\n",
        "    \n",
        "    cleaned_count = df_cleaned.count()\n",
        "    original_count = df_raw_events.count()\n",
        "    \n",
        "    print(f'‚úÖ Data Cleaning Completed:')\n",
        "    print(f'   üì• Original Records: {original_count:,}')\n",
        "    print(f'   üì§ Cleaned Records: {cleaned_count:,}')\n",
        "    print(f'   üóëÔ∏è  Records Removed: {original_count - cleaned_count:,} ({((original_count - cleaned_count) / original_count * 100):.2f}%)')\n",
        "    \n",
        "    logger.info(f'‚úÖ Data cleaning completed - {cleaned_count:,} clean records')\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f'‚ùå Data cleaning failed: {str(e)}')\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-5-cleaning-explanation"
        ]
      },
      "source": [
        "#### üßπ Data Cleaning Techniques Explained\n\n",
        "<div style='background-color: #f8f9fa; padding: 15px; border-left: 4px solid #6c757d;'>\n",
        "<h5>üîç Cleaning Operations Breakdown</h5>\n",
        "<p>The data cleaning pipeline above implements several critical operations:</p>\n",
        "</div>\n\n",
        "**1. Null Value Filtering:**\n",
        "```python\n",
        ".filter(col('customer_id').isNotNull())  # Remove records without customer ID\n",
        ".filter(col('event_date').isNotNull())   # Ensure all events have dates\n",
        "```\n\n",
        "**2. Business Rule Validation:**\n",
        "```python\n",
        ".filter(col('revenue') >= 0)  # No negative revenue values\n",
        "```\n\n",
        "**3. Feature Engineering:**\n",
        "```python\n",
        ".withColumn('year_month', date_format(col('event_date'), 'yyyy-MM'))  # Time grouping\n",
        ".withColumn('revenue_bucket', when(...))  # Categorical segmentation\n",
        "```\n\n",
        "**4. Data Standardization:**\n",
        "```python\n",
        ".withColumn('acquisition_channel', coalesce(col('channel'), lit('unknown')))\n",
        "# Handles null channels with default value\n",
        "```\n\n",
        "**5. Deduplication:**\n",
        "```python\n",
        ".dropDuplicates(['customer_id', 'event_type', 'event_date'])\n",
        "# Removes duplicate events for same customer on same day\n",
        "```\n\n",
        "üí° **Pro Tip:** Always measure data loss during cleaning to ensure you're not removing valid business data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "task-5-metrics"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Task 5: Customer Acquisition Metrics Calculation\n",
        "# Calculate CAC, LTV, and conversion metrics for Amazon's customer acquisition platform\n",
        "\n",
        "try:\n",
        "    logger.info('üìä Calculating customer acquisition metrics')\n",
        "    \n",
        "    # Calculate Customer Acquisition Cost (CAC) by channel and time period\n",
        "    df_cac_metrics = df_cleaned \\\n",
        "        .filter(col('event_type') == 'acquisition') \\\n",
        "        .groupBy('acquisition_channel', 'clean_campaign_id', 'year_month') \\\n",
        "        .agg(\n",
        "            sum('marketing_spend').alias('total_marketing_spend'),\n",
        "            countDistinct('customer_id').alias('customers_acquired'),\n",
        "            avg('marketing_spend').alias('avg_spend_per_acquisition')\n",
        "        ) \\\n",
        "        .withColumn('cac', \n",
        "            when(col('customers_acquired') > 0, \n",
        "                 col('total_marketing_spend') / col('customers_acquired')\n",
        "            ).otherwise(0)\n",
        "        ) \\\n",
        "        .withColumn('calculated_timestamp', current_timestamp())\n",
        "    \n",
        "    # Calculate Customer Lifetime Value (LTV) metrics\n",
        "    df_ltv_metrics = df_cleaned \\\n",
        "        .filter(col('event_type') == 'purchase') \\\n",
        "        .groupBy('customer_id', 'acquisition_channel') \\\n",
        "        .agg(\n",
        "            sum('revenue').alias('total_revenue'),\n",
        "            count('*').alias('purchase_frequency'),\n",
        "            avg('revenue').alias('avg_order_value'),\n",
        "            min('event_date').alias('first_purchase_date'),\n",
        "            max('event_date').alias('last_purchase_date')\n",
        "        ) \\\n",
        "        .withColumn('customer_tenure_days',\n",
        "            greatest(datediff(col('last_purchase_date'), col('first_purchase_date')), lit(1))\n",
        "        ) \\\n",
        "        .withColumn('ltv_12_month_estimate', \n",
        "            col('total_revenue') * (365.0 / col('customer_tenure_days'))\n",
        "        ) \\\n",
        "        .withColumn('customer_segment',\n",
        "            when(col('total_revenue') >= 1000, 'high_value')\n",
        "            .when(col('total_revenue') >= 500, 'medium_value')\n",
        "            .otherwise('low_value')\n",
        "        )\n",
        "    \n",
        "    # Calculate conversion rates by channel and time period\n",
        "    df_conversion_metrics = df_cleaned \\\n",
        "        .groupBy('acquisition_channel', 'year_month') \\\n",
        "        .agg(\n",
        "            countDistinct('customer_id').alias('total_customers'),\n",
        "            countDistinct(\n",
        "                when(col('event_type') == 'purchase', col('customer_id'))\n",
        "            ).alias('converted_customers')\n",
        "        ) \\\n",
        "        .withColumn('conversion_rate_pct',\n",
        "            when(col('total_customers') > 0,\n",
        "                 (col('converted_customers') / col('total_customers')) * 100\n",
        "            ).otherwise(0)\n",
        "        )\n",
        "    \n",
        "    # Cache metrics for performance\n",
        "    df_cac_metrics.cache()\n",
        "    df_ltv_metrics.cache()\n",
        "    df_conversion_metrics.cache()\n",
        "    \n",
        "    print('‚úÖ Customer Acquisition Metrics Calculated Successfully')\n",
        "    \n",
        "    logger.info('‚úÖ Customer acquisition metrics calculation completed')\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f'‚ùå Metrics calculation failed: {str(e)}')\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-5-metrics-explanation"
        ]
      },
      "source": [
        "#### üìä Customer Acquisition Metrics Deep Dive\n\n",
        "<div style='background-color: #e8f5e8; padding: 15px; border-left: 4px solid #28a745;'>\n",
        "<h5>üí∞ Business Metrics Explained</h5>\n",
        "<p>Understanding these key customer acquisition metrics is crucial for business success:</p>\n",
        "</div>\n\n",
        "**1. Customer Acquisition Cost (CAC):**\n",
        "```\n",
        "CAC = Total Marketing Spend / Number of Customers Acquired\n",
        "```\n",
        "- Measures efficiency of marketing investments\n",
        "- Lower CAC = more efficient acquisition\n",
        "- Should be tracked by channel and campaign\n\n",
        "**2. Customer Lifetime Value (LTV):**\n",
        "```\n",
        "LTV = Total Revenue √ó (365 / Customer Tenure Days)\n",
        "```\n",
        "- Estimates total revenue from a customer over 12 months\n",
        "- Higher LTV = more valuable customers\n",
        "- Used to justify marketing investments\n\n",
        "**3. LTV/CAC Ratio:**\n",
        "```\n",
        "LTV/CAC Ratio = Average LTV / Average CAC\n",
        "```\n",
        "- **> 3.0:** Healthy acquisition channel\n",
        "- **< 1.0:** Losing money on acquisitions\n",
        "- **Optimal:** Between 3.0 and 5.0\n\n",
        "**4. Conversion Rate:**\n",
        "```\n",
        "Conversion Rate = (Converted Customers / Total Customers) √ó 100\n",
        "```\n",
        "- Measures funnel efficiency\n",
        "- Higher conversion = better targeting\n\n",
        "### üéØ Advanced PySpark Aggregation Patterns\n",
        "```python\n",
        "# Window functions for cohort analysis\n",
        ".withColumn('customer_rank', row_number().over(Window.partitionBy('customer_id').orderBy('event_date')))\n\n",
        "# Conditional aggregations\n",
        "countDistinct(when(col('event_type') == 'purchase', col('customer_id')))\n\n",
        "# Percentile calculations\n",
        "percentile_approx('revenue', 0.5).alias('median_revenue')\n",
        "```\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "task-5-results"
        ]
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Task 5: Display Transformation Results\n",
        "# Show calculated metrics and key insights\n",
        "\n",
        "print('üìä CUSTOMER ACQUISITION COST (CAC) BY CHANNEL:')\n",
        "print('=' * 60)\n",
        "df_cac_metrics.orderBy(desc('cac')).show(20, truncate=False)\n",
        "\n",
        "print('\\nüìà CUSTOMER LIFETIME VALUE (LTV) SUMMARY:')\n",
        "print('=' * 60)\n",
        "df_ltv_metrics.agg(\n",
        "    count('customer_id').alias('total_customers'),\n",
        "    avg('total_revenue').alias('avg_ltv'),\n",
        "    percentile_approx('total_revenue', 0.5).alias('median_ltv'),\n",
        "    max('total_revenue').alias('max_ltv'),\n",
        "    avg('purchase_frequency').alias('avg_purchases_per_customer')\n",
        ").show(truncate=False)\n",
        "\n",
        "print('\\nüéØ CONVERSION RATES BY CHANNEL:')\n",
        "print('=' * 60)\n",
        "df_conversion_metrics.orderBy(desc('conversion_rate_pct')).show(20, truncate=False)\n",
        "\n",
        "print('\\nüí∞ LTV/CAC RATIO BY CHANNEL (Key Business Metric):')\n",
        "print('=' * 60)\n",
        "# Calculate LTV/CAC ratio for business insights\n",
        "ltv_by_channel = df_ltv_metrics.groupBy('acquisition_channel').agg(\n",
        "    avg('ltv_12_month_estimate').alias('avg_ltv_12m')\n",
        ")\n",
        "\n",
        "cac_by_channel = df_cac_metrics.groupBy('acquisition_channel').agg(\n",
        "    avg('cac').alias('avg_cac')\n",
        ")\n",
        "\n",
        "ltv_cac_ratio = ltv_by_channel.join(cac_by_channel, 'acquisition_channel', 'inner') \\\n",
        "    .withColumn('ltv_cac_ratio', \n",
        "        when(col('avg_cac') > 0, col('avg_ltv_12m') / col('avg_cac')).otherwise(0)\n",
        "    ) \\\n",
        "    .orderBy(desc('ltv_cac_ratio'))\n",
        "\n",
        "ltv_cac_ratio.show(truncate=False)\n",
        "\n",
        "print('\\nüìã Business Insights:')\n",
        "print('   ‚Ä¢ LTV/CAC ratio > 3.0 indicates healthy acquisition channels')\n",
        "print('   ‚Ä¢ Focus investment on channels with highest LTV/CAC ratios')\n",
        "print('   ‚Ä¢ Monitor conversion rates and optimize underperforming channels')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "task-5-summary"
        ]
      },
      "source": [
        "### ‚úÖ Task 5 Completed\n\n",
        "Task 'Task 5' has been executed successfully.\n",
        "Check the output above for results and any validation messages.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary üìà\n\n",
        "This notebook completed 5 tasks for customer acquisition analytics:\n\n",
        "- **Task 1:** Develop a PySpark job to read and consolidate multiple large CSV files containing customer transaction data into a single DataFrame. Implement robust error handling and logging for file read operations, and ensure schema consistency across files.\n- **Task 2:** Implement data cleaning steps (e.g., handling missing values, removing duplicates, correcting data types) and perform feature engineering, including calculation of RFM (Recency, Frequency, Monetary) metrics and transaction pattern features using PySpark transformations.\n- **Task 3:** Create PySpark routines to validate data quality (e.g., null checks, range checks, uniqueness constraints) and generate summary statistics and data profiling reports. Log all validation results and flag any data quality issues for review.\n- **Task 4:** After applying the churn prediction model, export the resulting DataFrame with customer churn risk scores to Parquet format for downstream ML consumption. Ensure partitioning and efficient storage for large datasets.\n- **Task 5:** Develop comprehensive tests for each ETL pipeline stage using PySpark, including data ingestion, transformation, validation, and output. Implement detailed logging and error handling to monitor pipeline execution and facilitate troubleshooting.",
        "\n\n**Next Steps:**\n",
        "- Monitor job execution in Databricks\n",
        "- Validate data quality metrics\n",
        "- Set up automated scheduling\n",
        "- Configure alerts for pipeline failures\n\n",
        "**Production Ready:** ‚úÖ Optimized for Amazon's customer acquisition platform"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (PySpark)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}